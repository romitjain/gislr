{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772795d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b710d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_SIZE = (512, 512, 3)\n",
    "VAL_SIZE = 0.2\n",
    "\n",
    "train_dataset_path = \"train_landmark_files/\"\n",
    "save_dataset_path = \"test/\"\n",
    "class_path = \"train.csv\"\n",
    "class_to_label_path = \"sign_to_prediction_index_map.json\"\n",
    "train_data_with_label = \"train_data_with_label.csv\"\n",
    "val_participants = [55372, 61333, 62590]\n",
    "\n",
    "log_dir = os.path.join(\"./logs/fit/\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir, histogram_freq=1\n",
    ")\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(f'./model/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")}', 'cp-{epoch:02d}'),\n",
    "    save_freq='epoch',\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_period=128\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset(class_path: str, class_to_label_path: str) -> pd.DataFrame:\n",
    "    class_name = pd.read_csv(class_path)\n",
    "\n",
    "    with open(class_to_label_path, \"r\") as fp:\n",
    "        y_label = json.load(fp)\n",
    "        y_label = pd.DataFrame.from_dict(y_label, orient=\"index\", columns = [\"y_label\"])\n",
    "\n",
    "    df = class_name.set_index(\"sign\").join(y_label).reset_index()\n",
    "    df[\"save_dataset_path\"] = df.sequence_id.apply(lambda x: os.path.join(save_dataset_path + str(x) + \".npy\"))\n",
    "    df[\"example_set\"] = df.participant_id.apply(lambda x: \"val\" if x in val_participants else \"train\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb20ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_train_dataset(class_path, class_to_label_path)\n",
    "print(df[df.example_set == \"train\"].y_label.nunique(), df[df.example_set == \"val\"].y_label.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f83affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2af059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GISLRSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(\n",
    "            self, df: pd.DataFrame,\n",
    "            x_col: str,\n",
    "            y_col: str,\n",
    "            sample_size: int,\n",
    "            batch_size: int,\n",
    "            shuffle: bool = True\n",
    "        ) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self.sample_size = sample_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_classes = df[y_col].nunique()\n",
    "\n",
    "        self.x_files_path = df[x_col]\n",
    "        self.y = df[y_col]\n",
    "\n",
    "        self.indices = df.index.to_list()\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.x_files_path) // self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subset = self.indices[(idx * self.batch_size):((idx + 1) * self.batch_size)]\n",
    "        batch_x = self.x_files_path[subset]\n",
    "        batch_y = self.y[subset]\n",
    "\n",
    "        X, y = self._get_data(batch_x, batch_y)\n",
    "\n",
    "        return X, tf.one_hot(y, depth=self.n_classes)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def _get_data(self, batch_x, batch_y):\n",
    "        temp_X, temp_y = [], []\n",
    "\n",
    "        for idx, idy in zip(batch_x, batch_y):\n",
    "            temp = self._get_transformed_data(idx)\n",
    "\n",
    "            temp_X.append(temp)\n",
    "            temp_y.append(idy)\n",
    "\n",
    "        return np.array(temp_X), np.array(temp_y)\n",
    "\n",
    "    def _get_transformed_data(self, dataset_file_path) -> np.ndarray:\n",
    "        all_imgs = np.load(dataset_file_path)\n",
    "        logger.info(f\"Found: {all_imgs.shape} frames in {dataset_file_path}\")\n",
    "\n",
    "        og_size = all_imgs.shape[0]\n",
    "        bfill = True if np.random.uniform() < 0.5 else False\n",
    "        for i in range(0, self.sample_size-og_size):\n",
    "            frame_to_add = i//og_size\n",
    "            if bfill:\n",
    "                frame_to_add = abs(og_size-i)//og_size\n",
    "\n",
    "            all_imgs = np.concatenate((\n",
    "                all_imgs, np.expand_dims(all_imgs[frame_to_add], axis = 0)\n",
    "            ))\n",
    "\n",
    "        all_imgs = np.where(np.isnan(all_imgs), 0, all_imgs)\n",
    "\n",
    "        return all_imgs[:self.sample_size, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ba0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = GISLRSequence(\n",
    "    df[df.example_set == \"train\"],\n",
    "    x_col=\"save_dataset_path\",\n",
    "    y_col=\"y_label\",\n",
    "    sample_size=64,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "for x, y in test:\n",
    "    break\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a66f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GISLRSequence(\n",
    "    df[df.example_set == \"train\"],\n",
    "    x_col=\"save_dataset_path\",\n",
    "    y_col=\"y_label\",\n",
    "    sample_size=32,\n",
    "    batch_size = 16\n",
    ")\n",
    "\n",
    "val_dataset = GISLRSequence(\n",
    "    df[df.example_set == \"val\"],\n",
    "    x_col=\"save_dataset_path\",\n",
    "    y_col=\"y_label\",\n",
    "    sample_size=32,\n",
    "    batch_size = 16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c664dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x\n",
    "        )\n",
    "        x = self.add([x, attn_output])\n",
    "        return attn_output\n",
    "\n",
    "class GISLRModelv2():\n",
    "    def __init__(self, n_classes: int, input_shape = (32, 1629)) -> None:\n",
    "        model_input = tf.keras.Input(shape=input_shape)\n",
    "        # query_seq_encoding = tf.keras.layers.Conv1D(filters=128, kernel_size=4, padding='same')(model_input)\n",
    "        query_seq_encoding = tf.keras.layers.DepthwiseConv1D(kernel_size=4, padding='same', depth_multiplier=1)(model_input)\n",
    "\n",
    "        # mha = GlobalSelfAttention(num_heads=2, key_dim=128)(query_seq_encoding)\n",
    "        query_seq_encoding_bn = tf.keras.layers.BatchNormalization()(query_seq_encoding)\n",
    "        global_pool = tf.keras.layers.GlobalAveragePooling1D()(query_seq_encoding_bn)\n",
    "\n",
    "        dense_attn = tf.keras.layers.Dense(256, activation=\"relu\")(global_pool)\n",
    "        dropout = tf.keras.layers.Dropout(0.2)(dense_attn)\n",
    "\n",
    "        output = tf.keras.layers.Dense(\n",
    "            n_classes, activation=\"softmax\")(dropout)\n",
    "\n",
    "        self.model = tf.keras.Model(model_input, output)\n",
    "\n",
    "    def get_model(self):\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "            metrics=[tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.CategoricalCrossentropy()]\n",
    "        )\n",
    "        print(self.model.summary())\n",
    "\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GISLRModelv2(n_classes=df.y_label.nunique()).get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[tensorboard_callback, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339128b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9681723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
